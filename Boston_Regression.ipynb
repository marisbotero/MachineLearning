{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Boston_Regression.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/marisbotero/MachineLearning/blob/master/Boston_Regression.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_tm7wY2C7pPd"
      },
      "source": [
        "from __future__ import print_function\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "from sklearn.utils import shuffle\n",
        "\n",
        "# Importar la base de datos desde las utilidades de sklearn\n",
        "from sklearn.datasets import load_boston\n",
        "DataSet=load_boston()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v7bW0l7l8Iht"
      },
      "source": [
        "from google.colab import drive\n",
        "#drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lzCErjEF8J4n",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "522072bc-debf-4368-91db-05622e2af835"
      },
      "source": [
        "#%cd drive/My\\ Drive/Colab\\ Notebooks/Datasets\n",
        "#%cd /content/drive\n",
        "!ls"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "drive  sample_data\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_Pihga9F8iZk"
      },
      "source": [
        "\n",
        "# Define some parameters\n",
        "learning_rate = 0.001\n",
        "epochs = 10000\n",
        "batch_size = 25\n",
        "display_step = 50\n",
        "\n",
        "# Neural Network parameters (2 layer model)\n",
        "n_hidden_1 = 50 # 1st layer number of neurons\n",
        "n_hidden_2 = 30 # 2nd layer number of neurons\n",
        "n_input = 13\n",
        "n_classes = 1 # Just one output\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OJkoDEcU-lvf"
      },
      "source": [
        "\n",
        "# Dataset conditioning\n",
        "\n",
        "X, Y = shuffle(DataSet.data, DataSet.target)\n",
        "samples=Y.size\n",
        "Y=Y.reshape((samples,n_classes))\n",
        "\n",
        "\n",
        "\n",
        "offset = int(X.shape[0] * 0.80)\n",
        "\n",
        "X_train, Y_train = X[:offset], Y[:offset]\n",
        "X_test, Y_test = X[offset:], Y[offset:]\n",
        "\n",
        "Y_test  = np.asarray(Y_test)\n",
        "Y_train = np.asarray(Y_train)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JvWYUyKq-uKY",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "3c7bcbfc-0c87-41d8-cac1-a61051d5c352"
      },
      "source": [
        "# Create placeholders\n",
        "X = tf.placeholder(\"float\", [None, - ])\n",
        "Y = tf.placeholder(\"float\", [None, - ])\n",
        "\n",
        "'''\n",
        "Homework: Create a NN_Utils.py script with \"create_placeholders\" function to \n",
        "declare X and Y placeholders. e.g\n",
        "\n",
        "X, Y = create_placeholders(64, 64, 1, 1) \n",
        "'''\n",
        "\n",
        "# Create weights and biases\n",
        "weights = {\n",
        "           'h1' : tf.Variable(tf.truncated_normal([ - , - ], stddev=0.1)),\n",
        "           'h2' : tf.Variable(tf.truncated_normal([ - , - ], stddev=0.1)),\n",
        "           'out': tf.Variable(tf.truncated_normal([ - , - ], stddev=0.1))\n",
        "          }\n",
        "\n",
        "biases = {\n",
        "          'b1' : tf.Variable(tf.truncated_normal([ - ], stddev=0.1)),\n",
        "          'b2' : tf.Variable(tf.truncated_normal([ - ], stddev=0.1)),\n",
        "          'out': tf.Variable(tf.truncated_normal([ - ], stddev=0.1))\n",
        "         }\n",
        "\n",
        "'''\n",
        "Homework: Include \"initialize_parameters\" function within NN_Utils.py script \n",
        "to declare weights and biases. e.g\n",
        "\n",
        "parameters = initialize_parameters()\n",
        "'''\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nHomework: Include \"initialize_parameters\" function within NN_Utils.py script \\nto declare weights and biases. e.g\\n\\nparameters = initialize_parameters()\\n'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9OZCeUPH-wiJ"
      },
      "source": [
        "\n",
        "# Definición del perceptrón multicapa\n",
        "\n",
        "def multilayer_perceptron (x) :\n",
        "    \n",
        "    layer_1 = tf.add( tf.matmul ( - , - ), - ) # Hidden fully connected layer with n_hidden_1 neurons\n",
        "    layer_2 = tf.add( tf.matmul ( - , - ), - ) # Hidden fully connected layer with n_hidden_2 neurons\n",
        "    out_layer = tf.matmul( - , - ) + -  # Output fully connected layer with n_classes neurons\n",
        "    \n",
        "    return out_layer\n",
        "\n",
        "# Network outputs\n",
        "logits = multilayer_perceptron( - )\n",
        "\n",
        "# Create cost function\n",
        "cost = tf.reduce_mean(tf.losses.mean_squared_error ( predictions = - , labels = - ))\n",
        "\n",
        "'''\n",
        "Homework: Include \"compute_cost\" function within NN_Utils.py script \n",
        "to create cost function. e.g\n",
        "\n",
        "parameters = compute_cost()\n",
        "'''\n",
        "\n",
        "# Backpropagation\n",
        "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate) #.minimize(cost)\n",
        "optimizer = optimizer.minimize( - )\n",
        "\n",
        "# Initializing variables\n",
        "init = tf.global_variables_initializer()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zgA4xbn_-yKp",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 421
        },
        "outputId": "77547acf-ad26-4cde-9554-ae081e7bafe9"
      },
      "source": [
        "\n",
        "# Create a session\n",
        "with tf.Session() as sess:\n",
        "    sess.run(init)\n",
        "    \n",
        "    # Training epochs loop\n",
        "    for epoch in range(epochs):\n",
        "        avg_cost = 0.\n",
        "        # Number of groups to split training set \n",
        "        total_batch = int(Y_train.shape[0]/batch_size) \n",
        "        \n",
        "        # Training batches loop\n",
        "        for i in range(total_batch-1):\n",
        "            batch_x= X_train[i*batch_size:(i+1)*batch_size]\n",
        "            batch_y= Y_train[i*batch_size:(i+1)*batch_size]\n",
        "            \n",
        "            # Run optimization and cost function from feeding with batches\n",
        "            # dont return optimizer. Return cost in c\n",
        "            _ , c =sess.run( [optimizer, cost] , feed_dict = {X: - , Y: - } )\n",
        "            \n",
        "            # Cost average \n",
        "            avg_cost += c / total_batch\n",
        "            \n",
        "        # Show training results each display_step epochs\n",
        "        if epoch % display_step == 0:\n",
        "            print(\"Epoch:\", '%04d' % (epoch), \"cost = {:.5f}\".format(avg_cost))\n",
        "    \n",
        "    print(\"Optimization Finished!\")\n",
        "\n",
        "\n",
        "    # Testing stage \n",
        "    # Estimate accuracy through graph session\n",
        "    accuracy = tf.reduce_mean (tf.abs ( - - - ) )\n",
        "    \n",
        "    # Use logits.eval to extract logits after feeding with X_test adn Y_test\n",
        "    predictions = logits.eval( feed_dict =  {X: - , Y: - } )\n",
        "    \n",
        "    # Use accuracy.eval to extract accuracy after feeding with X_test adn Y_test\n",
        "    print(\"Test accuracy :\", accuracy.eval( feed_dict = {X: - , Y: - } ) )\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch: 0000 cost = 123.22601\n",
            "Epoch: 0050 cost = 37.10908\n",
            "Epoch: 0100 cost = 30.20278\n",
            "Epoch: 0150 cost = 26.64662\n",
            "Epoch: 0200 cost = 24.97230\n",
            "Epoch: 0250 cost = 24.12948\n",
            "Epoch: 0300 cost = 23.64710\n",
            "Epoch: 0350 cost = 23.33106\n",
            "Epoch: 0400 cost = 23.09891\n",
            "Epoch: 0450 cost = 22.91415\n",
            "Epoch: 0500 cost = 22.75945\n",
            "Epoch: 0550 cost = 22.62590\n",
            "Epoch: 0600 cost = 22.50862\n",
            "Epoch: 0650 cost = 22.40315\n",
            "Epoch: 0700 cost = 22.30929\n",
            "Epoch: 0750 cost = 22.23359\n",
            "Epoch: 0800 cost = 22.14406\n",
            "Epoch: 0850 cost = 22.06989\n",
            "Epoch: 0900 cost = 22.00243\n",
            "Epoch: 0950 cost = 21.93946\n",
            "Optimization Finished!\n",
            "Test accuracy : 3.9135263\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}