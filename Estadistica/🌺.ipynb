{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "🌺.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyMuRCydRBRCwaS98qrhclJg",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/marisbotero/MachineLearning/blob/master/%F0%9F%8C%BA.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fSxoGVpiav45"
      },
      "source": [
        "# **Como usar NLTK en Google Colab**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dPjZ3Ng3aNJW",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        },
        "outputId": "488b336f-80b3-446e-ee48-851f10e4d34a"
      },
      "source": [
        "#seleccionar download [d], luego descargar el recurso de nombre \"book\"\n",
        "import nltk\n",
        "nltk.download('cess_esp')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package cess_esp to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/cess_esp.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tiSiB04nbh5e"
      },
      "source": [
        "# **Expresiones Regulares**\n",
        "\n",
        "\n",
        "*   Constituyen un lenguaje estandarizado para definir cadenas de búsqueda de texto.\n",
        "*   Libreria de operaciones con  expresiones regulares de Python [re](https://docs.python.org/3/library/re.html)\n",
        "*   Reglas para escribir expresiones regulares [Wiki](https://es.wikipedia.org/wiki/Expresión_regular)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MDdHJjjYa6Pu",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        },
        "outputId": "f6ed90ab-4112-4ac8-aace-8e8db5583760"
      },
      "source": [
        "# spanish Corpus: https://mailman.uib.no/public/corpora/2007-October/005448.html\n",
        "import re\n",
        "corpus = nltk.corpus.cess_esp.sents() \n",
        "print(corpus)\n",
        "print(len(corpus))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[['El', 'grupo', 'estatal', 'Electricité_de_France', '-Fpa-', 'EDF', '-Fpt-', 'anunció', 'hoy', ',', 'jueves', ',', 'la', 'compra', 'del', '51_por_ciento', 'de', 'la', 'empresa', 'mexicana', 'Electricidad_Águila_de_Altamira', '-Fpa-', 'EAA', '-Fpt-', ',', 'creada', 'por', 'el', 'japonés', 'Mitsubishi_Corporation', 'para', 'poner_en_marcha', 'una', 'central', 'de', 'gas', 'de', '495', 'megavatios', '.'], ['Una', 'portavoz', 'de', 'EDF', 'explicó', 'a', 'EFE', 'que', 'el', 'proyecto', 'para', 'la', 'construcción', 'de', 'Altamira_2', ',', 'al', 'norte', 'de', 'Tampico', ',', 'prevé', 'la', 'utilización', 'de', 'gas', 'natural', 'como', 'combustible', 'principal', 'en', 'una', 'central', 'de', 'ciclo', 'combinado', 'que', 'debe', 'empezar', 'a', 'funcionar', 'en', 'mayo_del_2002', '.'], ...]\n",
            "6030\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s4qZ84LCdNk1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        },
        "outputId": "efa72acb-c428-498f-e60a-3b80a1cc94c7"
      },
      "source": [
        "#for l in corpus:\n",
        "  #for w in l\n",
        "    ## \n",
        "\n",
        "flatten = [w for l in corpus for w in l]\n",
        "print(flatten[:20])\n",
        "print(len(flatten))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['El', 'grupo', 'estatal', 'Electricité_de_France', '-Fpa-', 'EDF', '-Fpt-', 'anunció', 'hoy', ',', 'jueves', ',', 'la', 'compra', 'del', '51_por_ciento', 'de', 'la', 'empresa', 'mexicana']\n",
            "192685\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GnnG4Yioitbp"
      },
      "source": [
        "### **Estructura de la funcion re.search()**\n",
        "```\n",
        "# Determina si el patron de búsqueda p esta contenido en la cadena s\n",
        "re.seach(p, s)\n",
        "```\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VCSPZlCFVVTf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "80cf2d03-75f1-49d3-b452-6852cebe271b"
      },
      "source": [
        "# Meta-caracteres básicos\n",
        "arr = [w for w in flatten if re.search('es', w)]\n",
        "arr[:5]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['estatal', 'jueves', 'empresa', 'centrales', 'francesa']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UP-Vtd62jGaP",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "d4131905-f7e3-4870-9231-85ace1ca8086"
      },
      "source": [
        "arr = [w for w in flatten if re.search('es$', w)]\n",
        "arr[:5]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['jueves', 'centrales', 'millones', 'millones', 'dólares']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xZ08vKvKjRQU",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "78d569d4-7108-478d-8884-dbb38f525940"
      },
      "source": [
        "arr = [w for w in flatten if re.search('^es', w)]\n",
        "arr[:5]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['estatal', 'es', 'esta', 'esta', 'eso']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QltWUUQhj0as",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "90b94d53-449c-484e-fb63-7e5619050451"
      },
      "source": [
        "arr = [w for w in flatten if re.search('^..j..t..$', w)]\n",
        "arr"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['tajantes']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zhPectTIj7KQ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "57fc55c2-2260-434b-df0c-11120fa7839c"
      },
      "source": [
        "#Rangos [a-z], [A-Z], [0-9]\n",
        "arr = [w for w in flatten if re.search('^[ghi][mno][jlk][def]$', w)]\n",
        "arr"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['golf', 'golf']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "63AMmVplxmoX",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 191
        },
        "outputId": "6e52d96a-f0bc-492e-f6c4-6ef773694953"
      },
      "source": [
        "#Clausuras *, * (Kleene closures)\n",
        "arr = [w for w in flatten if re.search('^(no)*', w)]\n",
        "arr[:10]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['El',\n",
              " 'grupo',\n",
              " 'estatal',\n",
              " 'Electricité_de_France',\n",
              " '-Fpa-',\n",
              " 'EDF',\n",
              " '-Fpt-',\n",
              " 'anunció',\n",
              " 'hoy',\n",
              " ',']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4rgucyYHmhv2"
      },
      "source": [
        "## **Normalización de Texto** (como aplicación de las expresiones regulares)\n",
        "\n",
        "## **Tokenización:** Es el proceso mediante el cual se sub-divide una cadena de texto en unidades linguísticas minimas (palabras)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S0qOTxZqmTTu",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        },
        "outputId": "08084ec9-f34e-4f0a-cb85-ae6032202b38"
      },
      "source": [
        "texto = \"\"\" siempre he sido de amores anacrónicos: por esto mi obsesión con las cartas. \n",
        "Hay algo sobre la necesidad, la urgencia con la que se escriben y la intimidad que encierran, \n",
        "casi como si cada palabra fuera escrita a modo de confesión. \n",
        "José Cedeño dice que escribimos cartas por dos razones:\n",
        "“para que un ‘otro’ lea lo que no sabe, y para que uno se desprenda de lo que siempre supo” y yo le creo, \n",
        "pero también creo firmemente que son mucho más que eso. \n",
        "Lejos de ser un medio de comunicación pasado de moda, \n",
        "las cartas son un boceto del momento presente y aunque vienen de una experiencia personal, \n",
        "pueden llegar a reflejar un sentimiento colectivo. \n",
        "Vernos reflejados en las palabras ajenas (o más bien prestadas) \n",
        "hace que entendamos que en el fondo todos somos iguales; \n",
        "somos humanos y navegamos sobre el mismo miedo. Por esto hago este proyecto son cartas sin remitente ni destinatario \n",
        "(para que quien las encuentre, las use como vea necesario) \n",
        "como una invitación a construir puentes de palabras. \n",
        "Ojalá en ellas se encuentren y \n",
        "nos demos cuenta de que en medio de la soledad, no estamos tan solos. ...\"\"\"\n",
        "print(texto)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            " siempre he sido de amores anacrónicos: por esto mi obsesión con las cartas. \n",
            "Hay algo sobre la necesidad, la urgencia con la que se escriben y la intimidad que encierran, \n",
            "casi como si cada palabra fuera escrita a modo de confesión. \n",
            "José Cedeño dice que escribimos cartas por dos razones:\n",
            "“para que un ‘otro’ lea lo que no sabe, y para que uno se desprenda de lo que siempre supo” y yo le creo, \n",
            "pero también creo firmemente que son mucho más que eso. \n",
            "Lejos de ser un medio de comunicación pasado de moda, \n",
            "las cartas son un boceto del momento presente y aunque vienen de una experiencia personal, \n",
            "pueden llegar a reflejar un sentimiento colectivo. \n",
            "Vernos reflejados en las palabras ajenas (o más bien prestadas) \n",
            "hace que entendamos que en el fondo todos somos iguales; \n",
            "somos humanos y navegamos sobre el mismo miedo. Por esto hago este proyecto son cartas sin remitente ni destinatario \n",
            "(para que quien las encuentre, las use como vea necesario) \n",
            "como una invitación a construir puentes de palabras. \n",
            "Ojalá en ellas se encuentren y \n",
            "nos demos cuenta de que en medio de la soledad, no estamos tan solos. ...\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DdLIv_YgmvLl",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "b4a8a925-06fc-4bb9-e075-5dfb83a57ce5"
      },
      "source": [
        "# Caso 1: tokenizacion más simple: por espacios vacios !\n",
        "print(re.split(r' ', texto))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['', 'Fuimos', 'una', 'novela', 'que', 'tan\\n', 'solo', 'pudo', 'ser', 'poema', '...']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nAiJfS7Lm31V",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "d90fa983-7340-41c6-f7fc-64080c2d8160"
      },
      "source": [
        "# Caso 2: tokenización usando expresiones regulares\n",
        "print(re.split(r'[ \\t\\n]+', texto))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['', 'Fuimos', 'una', 'novela', 'que', 'tan', 'solo', 'pudo', 'ser', 'poema', '...']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jkkBYLzSm7Qt",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        },
        "outputId": "aecb6dd1-37dc-40f3-9a4d-02d80208eba5"
      },
      "source": [
        "# RegEx reference: \\W -> all characters other than letters, digits or underscore\n",
        "print(re.split(r'[ \\W\\t\\n]+', texto))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['', 'Cuando', 'sea', 'el', 'rey', 'del', 'mundo', 'imaginaba', 'él', 'en', 'su', 'cabeza', 'no', 'tendré', 'que', 'preocuparme', 'por', 'estas', 'bobadas', 'Era', 'solo', 'un', 'niño', 'de', '7', 'años', 'pero', 'pensaba', 'que', 'podría', 'ser', 'cualquier', 'cosa', 'que', 'su', 'imaginación', 'le', 'permitiera', 'visualizar', 'en', 'su', 'cabeza', '']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nOgYEslS4fdM"
      },
      "source": [
        "## **Tokenizador de NLTK**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xLBTKB20m-3O",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "d5dec66b-ad66-49a1-fd79-a6643c23e3a2"
      },
      "source": [
        "# nuestra antigua regex no funciona en este caso: \n",
        "texto = 'como me consigo algún motivo pa que los amaneceres que tu ves'\n",
        "print(re.split(r'[ \\W\\t\\n]+', texto))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['En', 'los', 'E', 'U', 'esa', 'postal', 'vale', '15', '50', '']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NQTXiZ-b4sO2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "9a493aad-d60d-4991-e075-3c1955851a18"
      },
      "source": [
        "pattern = r'''(?x)                 # set flag to allow verbose regexps\n",
        "              (?:[A-Z]\\.)+         # abbreviations, e.g. U.S.A.\n",
        "              | \\w+(?:-\\w+)*       # words with optional internal hyphens\n",
        "              | \\$?\\d+(?:\\.\\d+)?%? # currency and percentages, e.g. $12.40, 82%\n",
        "              | \\.\\.\\.             # ellipsis\n",
        "              | [][.,;\"'?():-_`]   # these are separate tokens; includes ], [\n",
        "'''\n",
        "nltk.regexp_tokenize(texto, pattern)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['En', 'los', 'E.U.', 'esa', 'postal', 'vale', '$15.50', '...']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J3Ac0Jol9rvk"
      },
      "source": [
        "## **Lematización:** Proceso para encontrar la raíz linguística de una palabra\n",
        "\n",
        "*   Derivación (stemming) : lematización simple"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pd701QnT5631",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        },
        "outputId": "c69899a8-e17b-4f16-c98a-7415f45c24e0"
      },
      "source": [
        "# Derivación simple\n",
        "from nltk import word_tokenize\n",
        "from nltk.stem.snowball import SnowballStemmer\n",
        "SnowballStemmer.languages"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('arabic',\n",
              " 'danish',\n",
              " 'dutch',\n",
              " 'english',\n",
              " 'finnish',\n",
              " 'french',\n",
              " 'german',\n",
              " 'hungarian',\n",
              " 'italian',\n",
              " 'norwegian',\n",
              " 'porter',\n",
              " 'portuguese',\n",
              " 'romanian',\n",
              " 'russian',\n",
              " 'spanish',\n",
              " 'swedish')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tm_cNzmg9vDm"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}