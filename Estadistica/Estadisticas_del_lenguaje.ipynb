{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Estadisticas_del_lenguaje.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyPRxBK46cz56h6i8mf+JkQo",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/marisbotero/MachineLearning/blob/master/Estadisticas_del_lenguaje.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4z4-0VyM-_8j"
      },
      "source": [
        "*Estadísticas básicas del lenguaje*\n",
        "\n",
        "# Configuración inicial\n",
        "\n",
        "* En este notebook vamos a descargar el dataset `book` que contiene muchos libros en ingles en formato de texto tokenizado, de los cuales vamos a hacer procesamiento."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8z7t7aHF_CSb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "dfad5356-9bbc-43f8-f806-41eb758676fb"
      },
      "source": [
        "import nltk \n",
        "nltk.download('book')\n",
        "from nltk.book import *\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading collection 'book'\n",
            "[nltk_data]    | \n",
            "[nltk_data]    | Downloading package abc to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/abc.zip.\n",
            "[nltk_data]    | Downloading package brown to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/brown.zip.\n",
            "[nltk_data]    | Downloading package chat80 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/chat80.zip.\n",
            "[nltk_data]    | Downloading package cmudict to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/cmudict.zip.\n",
            "[nltk_data]    | Downloading package conll2000 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/conll2000.zip.\n",
            "[nltk_data]    | Downloading package conll2002 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/conll2002.zip.\n",
            "[nltk_data]    | Downloading package dependency_treebank to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/dependency_treebank.zip.\n",
            "[nltk_data]    | Downloading package genesis to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/genesis.zip.\n",
            "[nltk_data]    | Downloading package gutenberg to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/gutenberg.zip.\n",
            "[nltk_data]    | Downloading package ieer to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/ieer.zip.\n",
            "[nltk_data]    | Downloading package inaugural to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/inaugural.zip.\n",
            "[nltk_data]    | Downloading package movie_reviews to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/movie_reviews.zip.\n",
            "[nltk_data]    | Downloading package nps_chat to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/nps_chat.zip.\n",
            "[nltk_data]    | Downloading package names to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/names.zip.\n",
            "[nltk_data]    | Downloading package ppattach to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/ppattach.zip.\n",
            "[nltk_data]    | Downloading package reuters to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package senseval to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/senseval.zip.\n",
            "[nltk_data]    | Downloading package state_union to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/state_union.zip.\n",
            "[nltk_data]    | Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data]    | Downloading package swadesh to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/swadesh.zip.\n",
            "[nltk_data]    | Downloading package timit to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/timit.zip.\n",
            "[nltk_data]    | Downloading package treebank to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/treebank.zip.\n",
            "[nltk_data]    | Downloading package toolbox to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/toolbox.zip.\n",
            "[nltk_data]    | Downloading package udhr to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/udhr.zip.\n",
            "[nltk_data]    | Downloading package udhr2 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/udhr2.zip.\n",
            "[nltk_data]    | Downloading package unicode_samples to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/unicode_samples.zip.\n",
            "[nltk_data]    | Downloading package webtext to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/webtext.zip.\n",
            "[nltk_data]    | Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/wordnet.zip.\n",
            "[nltk_data]    | Downloading package wordnet_ic to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/wordnet_ic.zip.\n",
            "[nltk_data]    | Downloading package words to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/words.zip.\n",
            "[nltk_data]    | Downloading package maxent_treebank_pos_tagger to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping taggers/maxent_treebank_pos_tagger.zip.\n",
            "[nltk_data]    | Downloading package maxent_ne_chunker to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping chunkers/maxent_ne_chunker.zip.\n",
            "[nltk_data]    | Downloading package universal_tagset to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping taggers/universal_tagset.zip.\n",
            "[nltk_data]    | Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data]    | Downloading package book_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping grammars/book_grammars.zip.\n",
            "[nltk_data]    | Downloading package city_database to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/city_database.zip.\n",
            "[nltk_data]    | Downloading package tagsets to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping help/tagsets.zip.\n",
            "[nltk_data]    | Downloading package panlex_swadesh to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    | Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
            "[nltk_data]    | \n",
            "[nltk_data]  Done downloading collection book\n",
            "*** Introductory Examples for the NLTK Book ***\n",
            "Loading text1, ..., text9 and sent1, ..., sent9\n",
            "Type the name of the text or sentence to view it.\n",
            "Type: 'texts()' or 'sents()' to list the materials.\n",
            "text1: Moby Dick by Herman Melville 1851\n",
            "text2: Sense and Sensibility by Jane Austen 1811\n",
            "text3: The Book of Genesis\n",
            "text4: Inaugural Address Corpus\n",
            "text5: Chat Corpus\n",
            "text6: Monty Python and the Holy Grail\n",
            "text7: Wall Street Journal\n",
            "text8: Personals Corpus\n",
            "text9: The Man Who Was Thursday by G . K . Chesterton 1908\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jJg03NqoFz13"
      },
      "source": [
        "# Analizando un texto"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g9aDd5Jz_0dA",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "b64440c6-38dc-48b8-8fec-18b872ecd577"
      },
      "source": [
        "# escogemos text1 que es el famoso libro Moby Dick\n",
        "text1"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<Text: Moby Dick by Herman Melville 1851>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gv06n-6WF2kU",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 191
        },
        "outputId": "b96d7dcc-1dfd-4e6e-a941-8128fd3927e1"
      },
      "source": [
        "# Vemos que el texto ya viene tokenizado incluyendo caracteres especiales ....\n",
        "text1.tokens[:10]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['[',\n",
              " 'Moby',\n",
              " 'Dick',\n",
              " 'by',\n",
              " 'Herman',\n",
              " 'Melville',\n",
              " '1851',\n",
              " ']',\n",
              " 'ETYMOLOGY',\n",
              " '.']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fYcg7AbzF7EW",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "fa8e7073-2da0-4e7a-9cbb-5ec1f1301a83"
      },
      "source": [
        "# ¿Cuantos tokens tiene el libro?\n",
        "len(text1)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "260819"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bK8sLGAtGH1N"
      },
      "source": [
        "## Medida de riqueza lexica en un texto: \n",
        "$$ R_l = \\frac{\\text{total de palabras únicas}}{\\text{total de palabras}} = \\frac{\\text{longitud del vocabulario}}{\\text{longitud del texto}}$$ "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WWApkNwkGFAt",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 884
        },
        "outputId": "a29d3c79-6a26-4c7e-de53-d11ecff244d3"
      },
      "source": [
        "# Primero realizamos la construcción de un vocabulario (identificamos las palabras unicas que hay en el libro)\n",
        "# https://docs.python.org/2/library/sets.html\n",
        "vocabulario = sorted(set(text1))\n",
        "vocabulario[1000:1050]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Crew',\n",
              " 'Crish',\n",
              " 'Crockett',\n",
              " 'Cross',\n",
              " 'Crossed',\n",
              " 'Crossing',\n",
              " 'Crotch',\n",
              " 'Crowding',\n",
              " 'Crown',\n",
              " 'Crozetts',\n",
              " 'Cruelty',\n",
              " 'Cruising',\n",
              " 'Cruppered',\n",
              " 'Crusaders',\n",
              " 'Crushed',\n",
              " 'Crying',\n",
              " 'Cuba',\n",
              " 'Curious',\n",
              " 'Curse',\n",
              " 'Cursed',\n",
              " 'Curses',\n",
              " 'Cussed',\n",
              " 'Customs',\n",
              " 'Cut',\n",
              " 'Cutter',\n",
              " 'Cutting',\n",
              " 'Cuvier',\n",
              " 'Cyclades',\n",
              " 'Czar',\n",
              " 'D',\n",
              " 'DAGGOO',\n",
              " 'DAM',\n",
              " 'DANCE',\n",
              " 'DANCING',\n",
              " 'DANIEL',\n",
              " 'DANISH',\n",
              " 'DARKENS',\n",
              " 'DARWIN',\n",
              " 'DAVENANT',\n",
              " 'DEAD',\n",
              " 'DEATH',\n",
              " 'DEBELL',\n",
              " 'DECK',\n",
              " 'DEL',\n",
              " 'DESTROYED',\n",
              " 'DEVIL',\n",
              " 'DICTIONARY',\n",
              " 'DID',\n",
              " 'DIGNITY',\n",
              " 'DISCOVERS']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U_yylcWjIGvY",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "b43f14f5-73c5-4602-f6f4-3f2396046759"
      },
      "source": [
        "# luego definimos la medida de riqueza léxica:\n",
        "rl = len(set(text1))/len(text1)\n",
        "print(rl)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.07406285585022564\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p-Ui0kBCOocY",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "04efd2b7-0d2f-4673-91d8-bb4dcdd296fc"
      },
      "source": [
        "# podemos definir funciones en python para estas medidas léxicas:\n",
        "def riqueza_lexica(texto):\n",
        "  return len(set(texto))/len(texto)\n",
        "\n",
        "def porcentaje_palabra(palabra, texto):\n",
        "  return 100*texto.count(palabra)/len(texto)\n",
        "\n",
        "riqueza_lexica(text1)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.07406285585022564"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5_2bEmS3PlP4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "3ce28a1c-3d1a-4439-82eb-39e1ec91ff15"
      },
      "source": [
        "# podemos calcular el porcentaje de texto que ocupa una palabra en una cadena larga de texto.\n",
        "porcentaje_palabra('monster', text1)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.018786974875296663"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LvkAxqUKPoT7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "05b921bc-1336-46f6-af13-9fc7d4cf6ced"
      },
      "source": [
        "# y calcular en conteo el numero de veces que aparece:\n",
        "text1.count('monster')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "49"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O7sEz90XPqW8"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}